}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (length(row(x[1])) > 1000){
return(num_of_points)
} else {
return(15)}}
sample1 <- read.csv("../Application/df1.csv")
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
library(dplyr)
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(1,2)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (length(row(x[1])) > 1000){
return(num_of_points)
} else {
return(15)}}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (length(row(x[1])) > 1000){
return(num_of_points)
} else {
return(15)}}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, 200)
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, 100)
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (length(row(x[1])) > 1000){
return(num_of_points)}
else if (between(nrow(x[1])), 10000, 20000){
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (length(row(x[1])) > 1000){
return(num_of_points)}
else if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*1.5}
else{
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*1.5}
else{
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*1.5}
else if (length(row(x[1])) > 1000){
return(num_of_points)}
else {
return(15)}
return(num_of_points)
}
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*0.5}
else if (length(row(x[1])) > 1000){
return(num_of_points)}
else {
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*0.8}
else if (length(row(x[1])) > 1000){
return(num_of_points)}
else {
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*0.7}
else if (length(row(x[1])) > 1000){
return(num_of_points)}
else {
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*1.5}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*0.65}
else if (length(row(x[1])) > 1000){
return(num_of_points)}
else {
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
gc()
df1_s1 <- list.files(path = "csvData/csvData1/Sample1/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
library(dplyr)
library(readr)
df1_s1 <- list.files(path = "csvData/csvData1/Sample1/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df1_s2 <- list.files(path = "csvData/csvData1/Sample2/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df1_s3 <- list.files(path = "csvData/csvData1/Sample3/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df1_s4 <- list.files(path = "csvData/csvData1/Sample4/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df2_s1 <- list.files(path = "csvData/csvData2/Sample1/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df2_s2 <- list.files(path = "csvData/csvData2/Sample2/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df2_s3 <- list.files(path = "csvData/csvData2/Sample3/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df2_s4 <- list.files(path = "csvData/csvData2/Sample4/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df3_s1 <- list.files(path = "csvData/csvData3/Sample1/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df3_s2 <- list.files(path = "csvData/csvData3/Sample2/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df4_s1 <- list.files(path = "csvData/csvData4/Sample1/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df4_s2 <- list.files(path = "csvData/csvData4/Sample2/", pattern = "*.csv", full.names = T) %>%
lapply(read_csv) %>%
bind_rows()
df1 <- rbind(df1_s1, df1_s2, df1_s3, df1_s4)
df2 <- rbind(df2_s1, df2_s2, df2_s3, df2_s4)
df3 <- rbind(df3_s1, df3_s2)
df4 <- rbind(df4_s1, df4_s2)
db_clustering <- function(file_in, NumOfPts = 20){
# Random samples data if file to big
if (between(nrow(file_in), 10000, 20000)){
file_in <- sample_n(file_in, 15000, replace = T)
memory.limit(size = 4000)
} else if (nrow(file_in) > 20000){
file_in <- sample_n(file_in, 25000, replace = T)
memory.limit(size = 4000)
}
# strip all columns except fsc and ssc
modified <- file_in[,c(2,3)]
# if there are NA's, replaces them with the mean
modified[modified == 0] <- NA
for(i in 1:ncol(modified)){
modified[is.na(modified[,i]), i] <- mean(modified[,i], na.rm = TRUE)
}
# log transform file
log_file <- log2(modified)
# Apply Hdbscan on log transformed data
scan <- hdbscan(log_file, NumOfPts)
# plots the data and return scan statistics
plot(log_file, col = scan$cluster +1, pch = 20)
return(scan)
}
points_calc <- function(x){
# calculate number of points with 1.5%
num_of_points <- length(x[,1])/100*1.5
# if data is small, apply smaller number of points
if (between(nrow(x), 10000, 20000)){
num_of_points <- 15000/100*0.8}
else if (length(row(x[1])) > 20000){
num_of_points <- 25000/100*0.65}
else if (length(row(x[1])) > 1000){
return(num_of_points)}
else {
return(15)}
return(num_of_points)
}
sample1 <- read.csv("../Application/df1.csv")
start_time <- Sys.time()
db_clustering(sample1, points_calc(sample1))
end_time <- Sys.time()
cat("\nRunning time = ", round(end_time - start_time, 2), "Seconds")
